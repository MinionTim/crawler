{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinionTim/crawler/blob/master/so_vits_svc_4_0_colab_fixed_thanks_goat_YeezyBeaver_checkmate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ngoHSEl5Ki9"
      },
      "source": [
        "Temp fix to colab , please follow what the below cells say"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZYyce2Sh5JFk"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BJB_op3P5Yjj"
      },
      "outputs": [],
      "source": [
        "#@title CELL 1\n",
        "!pip install pyworld==0.3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rWKxO-TG5Ys3"
      },
      "outputs": [],
      "source": [
        "#@title CELL 2\n",
        "!pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wf5cmn8_gzek"
      },
      "outputs": [],
      "source": [
        "#@title Run the above cells one by one first run CELL 1 wait till green tick appears then Run CELL 2 wait till green tick appears Then restart runtime and run all the cells as you used to do before (no need to run this cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "Zt_z4Y5Z5Y5u"
      },
      "outputs": [],
      "source": [
        "#@title Setup 1 (just run this once)\n",
        "import os\n",
        "import glob\n",
        "!git clone https://github.com/effusiveperiscope/so-vits-svc -b eff-4.0\n",
        "os.chdir('/content/so-vits-svc')\n",
        "# install requirements one-at-a-time to ignore exceptions\n",
        "!cat requirements.txt | xargs -n 1 pip install --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install praat-parselmouth\n",
        "!pip install ipywidgets\n",
        "!pip install huggingface_hub\n",
        "!pip install pip==23.0.1 # fix pip version for fairseq install\n",
        "!pip install fairseq==0.12.2\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "existing_files = glob.glob('/content/**/*.*', recursive=True)\n",
        "!pip install --upgrade protobuf==3.9.2\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UNq_7OZ-5Y_N"
      },
      "outputs": [],
      "source": [
        "#@title Setup 2 (just run this once)\n",
        "os.chdir('/content/so-vits-svc') # force working-directory to so-vits-svc - this line is just for safety and is probably not required\n",
        "\n",
        "import tarfile\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "# taken from https://github.com/CookiePPP/cookietts/blob/master/CookieTTS/utils/dataset/extract_unknown.py\n",
        "def extract(path):\n",
        "    if path.endswith(\".zip\"):\n",
        "        with ZipFile(path, 'r') as zipObj:\n",
        "           zipObj.extractall(os.path.split(path)[0])\n",
        "    elif path.endswith(\".tar.bz2\"):\n",
        "        tar = tarfile.open(path, \"r:bz2\")\n",
        "        tar.extractall(os.path.split(path)[0])\n",
        "        tar.close()\n",
        "    elif path.endswith(\".tar.gz\"):\n",
        "        tar = tarfile.open(path, \"r:gz\")\n",
        "        tar.extractall(os.path.split(path)[0])\n",
        "        tar.close()\n",
        "    elif path.endswith(\".tar\"):\n",
        "        tar = tarfile.open(path, \"r:\")\n",
        "        tar.extractall(os.path.split(path)[0])\n",
        "        tar.close()\n",
        "    elif path.endswith(\".7z\"):\n",
        "        import py7zr\n",
        "        archive = py7zr.SevenZipFile(path, mode='r')\n",
        "        archive.extractall(path=os.path.split(path)[0])\n",
        "        archive.close()\n",
        "    else:\n",
        "        raise NotImplementedError(f\"{path} extension not implemented.\")\n",
        "\n",
        "# taken from https://github.com/CookiePPP/cookietts/tree/master/CookieTTS/_0_download/scripts\n",
        "\n",
        "# megatools download urls\n",
        "win64_url = \"https://megatools.megous.com/builds/builds/megatools-1.11.1.20230212-win64.zip\"\n",
        "win32_url = \"https://megatools.megous.com/builds/builds/megatools-1.11.1.20230212-win32.zip\"\n",
        "linux_url = \"https://megatools.megous.com/builds/builds/megatools-1.11.1.20230212-linux-x86_64.tar.gz\"\n",
        "# download megatools\n",
        "from sys import platform\n",
        "import os\n",
        "import urllib.request\n",
        "import subprocess\n",
        "from time import sleep\n",
        "\n",
        "if platform == \"linux\" or platform == \"linux2\":\n",
        "        dl_url = linux_url\n",
        "elif platform == \"darwin\":\n",
        "    raise NotImplementedError('MacOS not supported.')\n",
        "elif platform == \"win32\":\n",
        "        dl_url = win64_url\n",
        "else:\n",
        "    raise NotImplementedError ('Unknown Operating System.')\n",
        "\n",
        "dlname = dl_url.split(\"/\")[-1]\n",
        "if dlname.endswith(\".zip\"):\n",
        "    binary_folder = dlname[:-4] # remove .zip\n",
        "elif dlname.endswith(\".tar.gz\"):\n",
        "    binary_folder = dlname[:-7] # remove .tar.gz\n",
        "else:\n",
        "    raise NameError('downloaded megatools has unknown archive file extension!')\n",
        "\n",
        "if not os.path.exists(binary_folder):\n",
        "    print('\"megatools\" not found. Downloading...')\n",
        "    if not os.path.exists(dlname):\n",
        "        urllib.request.urlretrieve(dl_url, dlname)\n",
        "    assert os.path.exists(dlname), 'failed to download.'\n",
        "    extract(dlname)\n",
        "    sleep(0.10)\n",
        "    os.unlink(dlname)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "binary_folder = os.path.abspath(binary_folder)\n",
        "\n",
        "def megadown(download_link, filename='.', verbose=False):\n",
        "    \"\"\"Use megatools binary executable to download files and folders from MEGA.nz .\"\"\"\n",
        "    filename = ' --path \"'+os.path.abspath(filename)+'\"' if filename else \"\"\n",
        "    wd_old = os.getcwd()\n",
        "    os.chdir(binary_folder)\n",
        "    try:\n",
        "        if platform == \"linux\" or platform == \"linux2\":\n",
        "            subprocess.call(f'./megatools dl{filename}{\" --debug http\" if verbose else \"\"} {download_link}', shell=True)\n",
        "        elif platform == \"win32\":\n",
        "            subprocess.call(f'megatools.exe dl{filename}{\" --debug http\" if verbose else \"\"} {download_link}', shell=True)\n",
        "    except:\n",
        "        os.chdir(wd_old) # don't let user stop download without going back to correct directory first\n",
        "        raise\n",
        "    os.chdir(wd_old)\n",
        "    return filename\n",
        "\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "import gdown\n",
        "from os.path import exists\n",
        "\n",
        "def request_url_with_progress_bar(url, filename):\n",
        "    class DownloadProgressBar(tqdm):\n",
        "        def update_to(self, b=1, bsize=1, tsize=None):\n",
        "            if tsize is not None:\n",
        "                self.total = tsize\n",
        "            self.update(b * bsize - self.n)\n",
        "    \n",
        "    def download_url(url, filename):\n",
        "        with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                                 miniters=1, desc=url.split('/')[-1]) as t:\n",
        "            filename, headers = urllib.request.urlretrieve(url, filename=filename, reporthook=t.update_to)\n",
        "            print(\"Downloaded to \"+filename)\n",
        "    download_url(url, filename)\n",
        "\n",
        "\n",
        "def download(urls, dataset='', filenames=None, force_dl=False, username='', password='', auth_needed=False):\n",
        "    assert filenames is None or len(urls) == len(filenames), f\"number of urls does not match filenames. Expected {len(filenames)} urls, containing the files listed below.\\n{filenames}\"\n",
        "    assert not auth_needed or (len(username) and len(password)), f\"username and password needed for {dataset} Dataset\"\n",
        "    if filenames is None:\n",
        "        filenames = [None,]*len(urls)\n",
        "    for i, (url, filename) in enumerate(zip(urls, filenames)):\n",
        "        print(f\"Downloading File from {url}\")\n",
        "        #if filename is None:\n",
        "        #    filename = url.split(\"/\")[-1]\n",
        "        if filename and (not force_dl) and exists(filename):\n",
        "            print(f\"{filename} Already Exists, Skipping.\")\n",
        "            continue\n",
        "        if 'drive.google.com' in url:\n",
        "            assert 'https://drive.google.com/uc?id=' in url, 'Google Drive links should follow the format \"https://drive.google.com/uc?id=1eQAnaoDBGQZldPVk-nzgYzRbcPSmnpv6\".\\nWhere id=XXXXXXXXXXXXXXXXX is the Google Drive Share ID.'\n",
        "            gdown.download(url, filename, quiet=False)\n",
        "        elif 'mega.nz' in url:\n",
        "            megadown(url, filename)\n",
        "        else:\n",
        "            #urllib.request.urlretrieve(url, filename=filename) # no progress bar\n",
        "            request_url_with_progress_bar(url, filename) # with progress bar\n",
        "\n",
        "import huggingface_hub\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "class HFModels:\n",
        "    def __init__(self, repo = \"therealvul/so-vits-svc-4.0\", \n",
        "            model_dir = \"hf_vul_models\"):\n",
        "        self.model_repo = huggingface_hub.Repository(local_dir=model_dir,\n",
        "            clone_from=repo, skip_lfs_files=True)\n",
        "        self.repo = repo\n",
        "        self.model_dir = model_dir\n",
        "\n",
        "        self.model_folders = os.listdir(model_dir)\n",
        "        self.model_folders.remove('.git')\n",
        "        self.model_folders.remove('.gitattributes')\n",
        "\n",
        "    def list_models(self):\n",
        "        return self.model_folders\n",
        "\n",
        "    # Downloads model;\n",
        "    # copies config to target_dir and moves model to target_dir\n",
        "    def download_model(self, model_name, target_dir):\n",
        "        if not model_name in self.model_folders:\n",
        "            raise Exception(model_name + \" not found\")\n",
        "        model_dir = self.model_dir\n",
        "        charpath = os.path.join(model_dir,model_name)\n",
        "\n",
        "        gen_pt = next(x for x in os.listdir(charpath) if x.startswith(\"G_\"))\n",
        "        cfg = next(x for x in os.listdir(charpath) if x.endswith(\"json\"))\n",
        "        try:\n",
        "          clust = next(x for x in os.listdir(charpath) if x.endswith(\"pt\"))\n",
        "        except StopIteration as e:\n",
        "          print(\"Note - no cluster model for \"+model_name)\n",
        "          clust = None\n",
        "\n",
        "        if not os.path.exists(target_dir):\n",
        "            os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        gen_dir = huggingface_hub.hf_hub_download(repo_id = self.repo,\n",
        "            filename = model_name + \"/\" + gen_pt) # this is a symlink\n",
        "        \n",
        "        if clust is not None:\n",
        "          clust_dir = huggingface_hub.hf_hub_download(repo_id = self.repo,\n",
        "              filename = model_name + \"/\" + clust) # this is a symlink\n",
        "          shutil.move(os.path.realpath(clust_dir), os.path.join(target_dir, clust))\n",
        "          clust_out = os.path.join(target_dir, clust)\n",
        "        else:\n",
        "          clust_out = None\n",
        "\n",
        "        shutil.copy(os.path.join(charpath,cfg),os.path.join(target_dir, cfg))\n",
        "        shutil.move(os.path.realpath(gen_dir), os.path.join(target_dir, gen_pt))\n",
        "\n",
        "        return {\"config_path\": os.path.join(target_dir,cfg),\n",
        "            \"generator_path\": os.path.join(target_dir,gen_pt),\n",
        "            \"cluster_path\": clust_out}\n",
        "\n",
        "# Example usage\n",
        "# vul_models = HFModels()\n",
        "# print(vul_models.list_models())\n",
        "# print(\"Applejack (singing)\" in vul_models.list_models())\n",
        "# vul_models.download_model(\"Applejack (singing)\",\"models/Applejack (singing)\")\n",
        "\n",
        "    print(\"Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r_3noVzm5ZFO"
      },
      "outputs": [],
      "source": [
        "#@title Download ContentVec (just run this once)\n",
        "os.chdir('/content/so-vits-svc') # force working-directory to so-vits-svc - this line is just for safety and is probably not required\n",
        "download([\"https://huggingface.co/therealvul/so-vits-svc-4.0-init/resolve/main/checkpoint_best_legacy_500.pt\"], filenames=[\"hubert/checkpoint_best_legacy_500.pt\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K9dkvLgl5ZKd"
      },
      "outputs": [],
      "source": [
        "#@title Other Downloads (.zip) Step o.1\n",
        "#@markdown Please note that 3.0 models are incompatible with 4.0.\n",
        "\n",
        "#@markdown Supported URL types: \n",
        "#@markdown * Google Drive zip\n",
        "#@markdown * MEGA zip\n",
        "#@markdown * Direct zip (+HuggingFace /resolve/ link)\n",
        "\n",
        "#@markdown Example URLs:\n",
        "#@markdown * Kanye: https://mega.nz/file/Dr40kCQI#G3bEWPvUvTa9SBJKQt7rETgcFds4ssnJF0nGN9aAXTk\n",
        "#@markdown * Kendrick: https://mega.nz/file/WmBzgSZa#UD-SFhHBv3aw0obTHW2lGc5yeaMnK8qtKU3OjDKMVKk\n",
        "#@markdown * Carti (v3): https://mega.nz/file/jnwzEJ4K#erlpUaNQ3VyQIIVaQDYge3Kv4pZtyNfQBWA6hUy6uu8\n",
        "#@markdown * Drake: https://mega.nz/file/Sm53wAwI#4PmIrSWDrEP1-pnZb5MJpTcfoHy3OBhBOhn2FVxfyb8\n",
        "#@markdown * Juice Wrld: https://mega.nz/file/5w9kGSJA#MQEQi7lBBBJMBa_rQ5mfGtDXnv96-XhhsNx-xc81ta8\n",
        "#@markdown * Travis: https://mega.nz/file/q652kCZb#VS9IE0Vr3A3PbynDvmkfantFmz_Iik9i3M9DMWeShoE\n",
        "#@markdown * Tyler: https://mega.nz/file/rz5SBBIK#KAhHX8tR-f5yf_aR4dRwF-oE90LpliA4E8v1YFC7ONQ\n",
        "#@markdown * AI Hub: https://discord.gg/Aktyxz4jwA\n",
        "\n",
        "\n",
        "import re\n",
        "model_url = \"https://mega.nz/file/jnwzEJ4K#erlpUaNQ3VyQIIVaQDYge3Kv4pZtyNfQBWA6hUy6uu8\" #@param {\"type\": \"string\"}\n",
        "if \"huggingface.co\" in model_url.lower():\n",
        "  download([re.sub(r\"/blob/\",\"/resolve/\",model_url)], \n",
        "           filenames=[os.path.join(os.getcwd(),model_url.split(\"/\")[-1])])\n",
        "else:\n",
        "  download([model_url])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2qpGPjDM5pIK"
      },
      "outputs": [],
      "source": [
        "#@title Extract .zip Downloads - Step o.2\n",
        "# download speaker model files into 'models' directory\n",
        "#import glob, os, shutil\n",
        "#from pathlib import Path\n",
        "#os.makedirs('models', exist_ok=True)\n",
        "#model_zip_paths = glob.glob('/content/**/*.zip', recursive=True)\n",
        "#for model_zip_path in model_zip_paths:\n",
        "#    extract(model_zip_path) # extract inplace\n",
        "#    ckpts_inplace = glob.glob('./*.pth')\n",
        "#    json_inplace = glob.glob('./*.json')\n",
        "#    if os.path.exists(os.path.splitext(model_zip_path)[0]):\n",
        "#      shutil.move(os.path.splitext(model_zip_path)[0],'models')\n",
        "#    elif len(ckpts_inplace):\n",
        "#      for f in ckpts_inplace:\n",
        "#        os.makedirs('models/'+Path(model_zip_path).stem, \n",
        "#                      exist_ok=True)\n",
        "#        shutil.move(f,'models/'+Path(model_zip_path).stem)\n",
        "#      for f in json_inplace:\n",
        "#        shutil.move(f,'models/'+Path(model_zip_path).stem)\n",
        "        #@title Extract .zip Downloads - Step o.2\n",
        "# download speaker model files into 'models' directory\n",
        "import glob, os, shutil\n",
        "from pathlib import Path\n",
        "os.makedirs('models', exist_ok=True)\n",
        "model_zip_paths = glob.glob('/content/**/*.zip', recursive=True)\n",
        "\n",
        "for model_zip_path in model_zip_paths:\n",
        "    print(\"extracting zip\",model_zip_path)\n",
        "    output_dir = os.path.join('/content/so-vits-svc/models',os.path.basename(os.path.splitext(model_zip_path)[0]).replace(\" \",\"_\"))\n",
        "    \n",
        "    # clean and create output dir\n",
        "    if os.path.exists(output_dir):\n",
        "      shutil.rmtree(output_dir)\n",
        "    os.mkdir(output_dir)\n",
        "    input_base = os.path.dirname(model_zip_path)\n",
        "\n",
        "    # clean input dir (if user stopped an earlier extract and we have dirty files)\n",
        "    ckpts_pre = glob.glob(os.path.join(input_base,'**/*.pth'),recursive=True)\n",
        "    jsons_pre = glob.glob(os.path.join(input_base,'**/config.json'),recursive=True)\n",
        "    for cpkt in ckpts_pre:\n",
        "      os.remove(cpkt)\n",
        "    for json in jsons_pre:\n",
        "      os.remove(json)\n",
        "\n",
        "    # do the extract\n",
        "    extract(model_zip_path)\n",
        "    ckpts = glob.glob(os.path.join(input_base,'**/*.pth'),recursive=True)\n",
        "    jsons = glob.glob(os.path.join(input_base,'**/config.json'),recursive=True)\n",
        "    for ckpt in ckpts:\n",
        "      shutil.move(ckpt,os.path.join(output_dir,os.path.basename(ckpt)))\n",
        "    for json in jsons:\n",
        "      shutil.move(json,os.path.join(output_dir,os.path.basename(json)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "x3ISGGIe5pVT"
      },
      "outputs": [],
      "source": [
        "#@title Open the file explorer on the left of your screen and drag-and-drop an audio file anywhere. Then run the below cell.\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import copy\n",
        "import logging\n",
        "import io\n",
        "from ipywidgets import widgets\n",
        "from pathlib import Path\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "os.chdir('/content/so-vits-svc')\n",
        "\n",
        "import torch\n",
        "from inference import infer_tool\n",
        "from inference import slicer\n",
        "from inference.infer_tool import Svc\n",
        "import soundfile\n",
        "import numpy as np\n",
        "\n",
        "MODELS_DIR = \"models\"\n",
        "\n",
        "def get_speakers():\n",
        "  speakers = []\n",
        "  for _,dirs,_ in os.walk(MODELS_DIR):\n",
        "    for folder in dirs:\n",
        "      cur_speaker = {}\n",
        "      # Look for G_****.pth\n",
        "      g = glob.glob(os.path.join(MODELS_DIR,folder,'G_*.pth'))\n",
        "      if not len(g):\n",
        "        print(\"Skipping \"+folder+\", no G_*.pth\")\n",
        "        continue\n",
        "      cur_speaker[\"model_path\"] = g[0]\n",
        "      cur_speaker[\"model_folder\"] = folder\n",
        "\n",
        "      # Look for *.pt (clustering model)\n",
        "      clst = glob.glob(os.path.join(MODELS_DIR,folder,'*.pt'))\n",
        "      if not len(clst):\n",
        "        print(\"Note: No clustering model found for \"+folder)\n",
        "        cur_speaker[\"cluster_path\"] = \"\"\n",
        "      else:\n",
        "        cur_speaker[\"cluster_path\"] = clst[0]\n",
        "\n",
        "      # Look for config.json\n",
        "      cfg = glob.glob(os.path.join(MODELS_DIR,folder,'*.json'))\n",
        "      if not len(cfg):\n",
        "        print(\"Skipping \"+folder+\", no config json\")\n",
        "        continue\n",
        "      cur_speaker[\"cfg_path\"] = cfg[0]\n",
        "      with open(cur_speaker[\"cfg_path\"]) as f:\n",
        "        try:\n",
        "          cfg_json = json.loads(f.read())\n",
        "        except Exception as e:\n",
        "          print(\"Malformed config json in \"+folder)\n",
        "        for name, i in cfg_json[\"spk\"].items():\n",
        "          cur_speaker[\"name\"] = name\n",
        "          cur_speaker[\"id\"] = i\n",
        "          if not name.startswith('.'):\n",
        "            speakers.append(copy.copy(cur_speaker))\n",
        "\n",
        "    return sorted(speakers, key=lambda x:x[\"name\"].lower())\n",
        "\n",
        "logging.getLogger('numba').setLevel(logging.WARNING)\n",
        "chunks_dict = infer_tool.read_temp(\"inference/chunks_temp.json\")\n",
        "existing_files = []\n",
        "slice_db = -40\n",
        "wav_format = 'wav'\n",
        "\n",
        "class InferenceGui():\n",
        "  def __init__(self):\n",
        "    self.speakers = get_speakers()\n",
        "    self.speaker_list = [x[\"name\"] for x in self.speakers]\n",
        "    self.speaker_box = widgets.Dropdown(\n",
        "        options = self.speaker_list\n",
        "    )\n",
        "    display(self.speaker_box)\n",
        "\n",
        "    def convert_cb(btn):\n",
        "      self.convert()\n",
        "    def clean_cb(btn):\n",
        "      self.clean()\n",
        "\n",
        "    self.convert_btn = widgets.Button(description=\"Convert\")\n",
        "    self.convert_btn.on_click(convert_cb)\n",
        "    self.clean_btn = widgets.Button(description=\"Delete all audio files\")\n",
        "    self.clean_btn.on_click(clean_cb)\n",
        "\n",
        "    self.trans_tx = widgets.IntText(value=0, description='Transpose')\n",
        "    self.cluster_ratio_tx = widgets.FloatText(value=0.0, \n",
        "      description='Clustering Ratio')\n",
        "    self.noise_scale_tx = widgets.FloatText(value=0.4, \n",
        "      description='Noise Scale')\n",
        "    self.auto_pitch_ck = widgets.Checkbox(value=False, description=\n",
        "      'Auto pitch f0 (do not use for singing)')\n",
        "\n",
        "    display(self.trans_tx)\n",
        "    display(self.cluster_ratio_tx)\n",
        "    display(self.noise_scale_tx)\n",
        "    display(self.auto_pitch_ck)\n",
        "    display(self.convert_btn)\n",
        "    display(self.clean_btn)\n",
        "\n",
        "  def convert(self):\n",
        "    trans = int(self.trans_tx.value)\n",
        "    speaker = next(x for x in self.speakers if x[\"name\"] == \n",
        "          self.speaker_box.value)\n",
        "    spkpth2 = os.path.join(os.getcwd(),speaker[\"model_path\"])\n",
        "    print(spkpth2)\n",
        "    print(os.path.exists(spkpth2))\n",
        "\n",
        "    svc_model = Svc(speaker[\"model_path\"], speaker[\"cfg_path\"], \n",
        "      cluster_model_path=speaker[\"cluster_path\"])\n",
        "    \n",
        "    input_filepaths = [f for f in glob.glob('/content/**/*.*', recursive=True)\n",
        "     if f not in existing_files and \n",
        "     any(f.endswith(ex) for ex in ['.wav','.flac','.mp3','.ogg','.opus'])]\n",
        "    for name in input_filepaths:\n",
        "      print(\"Converting \"+os.path.split(name)[-1])\n",
        "      infer_tool.format_wav(name)\n",
        "\n",
        "      wav_path = str(Path(name).with_suffix('.wav'))\n",
        "      wav_name = Path(name).stem\n",
        "      chunks = slicer.cut(wav_path, db_thresh=slice_db)\n",
        "      audio_data, audio_sr = slicer.chunks2audio(wav_path, chunks)\n",
        "\n",
        "      audio = []\n",
        "      for (slice_tag, data) in audio_data:\n",
        "          print(f'#=====segment start, '\n",
        "              f'{round(len(data)/audio_sr, 3)}s======')\n",
        "          \n",
        "          length = int(np.ceil(len(data) / audio_sr *\n",
        "              svc_model.target_sample))\n",
        "          \n",
        "          if slice_tag:\n",
        "              print('jump empty segment')\n",
        "              _audio = np.zeros(length)\n",
        "          else:\n",
        "              # Padding \"fix\" for noise\n",
        "              pad_len = int(audio_sr * 0.5)\n",
        "              data = np.concatenate([np.zeros([pad_len]),\n",
        "                  data, np.zeros([pad_len])])\n",
        "              raw_path = io.BytesIO()\n",
        "              soundfile.write(raw_path, data, audio_sr, format=\"wav\")\n",
        "              raw_path.seek(0)\n",
        "              _cluster_ratio = 0.0\n",
        "              if speaker[\"cluster_path\"] != \"\":\n",
        "                _cluster_ratio = float(self.cluster_ratio_tx.value)\n",
        "              out_audio, out_sr = svc_model.infer(\n",
        "                  speaker[\"name\"], trans, raw_path,\n",
        "                  cluster_infer_ratio = _cluster_ratio,\n",
        "                  auto_predict_f0 = bool(self.auto_pitch_ck.value),\n",
        "                  noice_scale = float(self.noise_scale_tx.value))\n",
        "              _audio = out_audio.cpu().numpy()\n",
        "              pad_len = int(svc_model.target_sample * 0.5)\n",
        "              _audio = _audio[pad_len:-pad_len]\n",
        "          audio.extend(list(infer_tool.pad_array(_audio, length)))\n",
        "          \n",
        "      res_path = os.path.join('/content/',\n",
        "          f'{wav_name}_{trans}_key_'\n",
        "          f'{speaker[\"name\"]}.{wav_format}')\n",
        "      soundfile.write(res_path, audio, svc_model.target_sample,\n",
        "          format=wav_format)\n",
        "      display(Audio(res_path, autoplay=True)) # display audio file\n",
        "    pass\n",
        "\n",
        "  def clean(self):\n",
        "     input_filepaths = [f for f in glob.glob('/content/**/*.*', recursive=True)\n",
        "     if f not in existing_files and \n",
        "     any(f.endswith(ex) for ex in ['.wav','.flac','.mp3','.ogg','.opus'])]\n",
        "     for f in input_filepaths:\n",
        "       os.remove(f)\n",
        "\n",
        "inference_gui = InferenceGui()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}